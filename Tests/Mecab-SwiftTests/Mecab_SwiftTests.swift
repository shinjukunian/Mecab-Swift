import XCTest
import Mecab_Swift
import CharacterFilter
import IPADic
import Dictionary
import IPADicDefinition

final class Mecab_SwiftTests: XCTestCase {
        
    
    var dictionaries:[DictionaryProviding]{
        return [IPADic()]
    }
    
    
    func testVersion(){
        let version=Tokenizer.version
        XCTAssertFalse(version.isEmpty)
    }
    
    func testLoading() {
        do{
            let tokenizer=try Tokenizer(dictionary: IPADic())
            XCTAssertNotNil(tokenizer)
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    func testLoadingFails() {
        do{
            let tokenizer=try Tokenizer(dictionary: IPADic())
            XCTAssertNotNil(tokenizer)
        }
        catch let error{
            XCTAssertFalse(error.localizedDescription.isEmpty)
        }
    }
    
    func testLoadingTokenization() {
        do{
            let tokenizer=try Tokenizer(dictionary: IPADic())
            XCTAssertNotNil(tokenizer)
            let string="ç†ŠãŒæ€–ã„ã§ã™ã€‚"
            let tokens=tokenizer.tokenize(text: string)
            XCTAssertFalse(tokens.isEmpty)
            print(tokens)
           
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    func testEnd() {
        do{
            let tokenizer=try Tokenizer(dictionary: IPADic())
            XCTAssertNotNil(tokenizer)
            let string="ç†ŠãŒæ€–ã„ã§ã™ã€‚"
            let annotations=tokenizer.tokenize(text: string).filter({$0.containsKanji}).compactMap({$0.furiganaAnnotation(for: string)})
            XCTAssertFalse(annotations.isEmpty)
            let expectedFurigana=["ãã¾","ã“ã‚"]
            for annotation in annotations{
                let subString=string[annotation.range]
                print("\(String(subString)), reading: \(annotation)")
            }
            for (expected,found) in zip(expectedFurigana,annotations.map({$0.reading})){
                XCTAssertEqual(expected, found)
            }
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    
    func testStart() {
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
            
                XCTAssertNotNil(tokenizer)
                let string="ãŠé›»è©±ãã ã•ã„ã€‚"
                let annotations=tokenizer.tokenize(text: string).filter({$0.containsKanji}).map({$0.furiganaAnnotation(for: string)})
                XCTAssertFalse(annotations.isEmpty)
                let expectedFurigana=["ã§ã‚“ã‚"]
                for annotation in annotations{
                    let subString=string[annotation.range]
                    print("\(String(subString)), reading: \(annotation)")
                }
                for (expected,found) in zip(expectedFurigana,annotations.map({$0.reading})){
                    XCTAssertEqual(expected, found)
                }
            }
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    func testCenter() {
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
                
                XCTAssertNotNil(tokenizer)
                let string="æ‰“ã¡ä¸Šã’ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼"
                let annotations=tokenizer.tokenize(text: string).filter({$0.containsKanji}).compactMap({$0.furiganaAnnotation(options: [.kanjiOnly], for: string)})
                XCTAssertFalse(annotations.isEmpty)
                let expectedFurigana=["ã†ã€€ã‚"]
                for annotation in annotations{
                    let subString=string[annotation.range]
                    print("\(String(subString)), reading: \(annotation)")
                }
                for (expected,found) in zip(expectedFurigana,annotations.map({$0.reading})){
                    XCTAssertEqual(expected, found)
                }
            }
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    
    func testTwoRanges() {
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
                XCTAssertNotNil(tokenizer)
                let string="ãŠçŸ¥ã‚‰ã›ã§ã™"
                let annotations=tokenizer.tokenize(text: string).filter({$0.containsKanji}).compactMap({$0.furiganaAnnotation(options: [.kanjiOnly], for: string)})
                XCTAssertFalse(annotations.isEmpty)
                let expectedFurigana=["ã—"]
                for annotation in annotations{
                    let subString=string[annotation.range]
                    print("\(String(subString)), reading: \(annotation)")
                }
                for (expected,found) in zip(expectedFurigana,annotations.map({$0.reading})){
                    XCTAssertEqual(expected, found)
                }
            }
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    func testTags(){
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
                XCTAssertNotNil(tokenizer)
                let string =
                 """
     JRæ±æ—¥æœ¬ã«ã‚ˆã‚‹ã¨ã€2æ—¥åˆå¾Œ7æ™‚15åˆ†ã”ã‚ã€JRæ–°å®¿é§…ã§ã€ç·šè·¯ã®ä¸Šã«ã„ãŸäººãŒã€å…¥ç·šã—ãŸå±±æ‰‹ç·šã¨è¡çªã—ãŸã€‚
     å¥³æ€§ã¨ã¿ã‚‰ã‚Œã€å®¹ä½“ã¯ä¸æ˜ã€‚
     
        ç¾å ´ã«ç™½æ–ãŒè½ã¡ã¦ãŠã‚Šã€è­¦è¦–åºãªã©ãŒèª¿ã¹ã¦ã„ã‚‹
     """
                
//                let rubyString=tokenizer.addRubyTags(to: string, transliteration: .hiragana, options: [.kanjiOnly])
                let rubyString=tokenizer.rubyTaggedString(source: string, transliteration: .hiragana, options: [.kanjiOnly])
                
                XCTAssertFalse(rubyString.isEmpty)
                XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
                
            }
            
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
        
    }
    
    func testLong(){
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
                XCTAssertNotNil(tokenizer)
                let string =
                    """
     æ„›çŸ¥çœŒã§é–‹å‚¬ä¸­ã®å›½éš›èŠ¸è¡“ç¥­ã€Œã‚ã„ã¡ãƒˆãƒªã‚¨ãƒ³ãƒŠãƒ¼ãƒ¬2019ã€ã®ä¼ç”»å±•ã€Œè¡¨ç¾ã®ä¸è‡ªç”±å±•ãƒ»ãã®å¾Œã€ã¯8æ—¥åˆå¾Œã«å†é–‹ã™ã‚‹ã€‚ä¸è‡ªç”±å±•ã¯å¾“è»æ…°å®‰å©¦ã‚’è±¡å¾´ã™ã‚‹ã€Œå¹³å’Œã®å°‘å¥³åƒã€ãªã©ã®å±•ç¤ºã«æŠ—è­°ãŒç›¸æ¬¡ãã€8æœˆ1æ—¥ã®èŠ¸è¡“ç¥­é–‹å¹•ã‹ã‚‰3æ—¥ã§ä¸­æ­¢ã«è¿½ã„è¾¼ã¾ã‚Œã¦ã„ãŸãŒã€èŠ¸è¡“ç¥­å®Ÿè¡Œå§”å“¡ä¼šä¼šé•·ã®å¤§æ‘ç§€ç« çŸ¥äº‹ãŒ7æ—¥ã€å®‰å…¨å¯¾ç­–ã‚„å…¥å ´åˆ¶é™ã‚’è¬›ã˜ãŸä¸Šã§å†é–‹ã™ã‚‹ã¨è¡¨æ˜ã—ãŸã€‚ã€å†™çœŸç‰¹é›†ã€‘å¤§å‹¢ã®æ¥å ´è€…ãŒé‘‘è³ã—ã¦ã„ãŸã€Œå¹³å’Œã®å°‘å¥³åƒã€èŠ¸è¡“ç¥­å®Ÿè¡Œå§”å“¡ä¼šã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã«ã‚ˆã‚‹ã¨ã€å…¥å ´ã¯1æ—¥2å›ã«åˆ¶é™ã—ã€æŠ½é¸ã§1å›30äººã«çµã‚Šã€1å›ç›®ã¯åˆå¾Œ2æ™‚10åˆ†ã‹ã‚‰1æ™‚é–“ã€‚2å›ç›®ã¯åˆå¾Œ4æ™‚20åˆ†ã‹ã‚‰40åˆ†é–“ã€‚å¤§æ‘çŸ¥äº‹ã®7æ—¥ã®ç™ºè¡¨ã«ã‚ˆã‚‹ã¨ã€é‘‘è³è€…ã¯äº‹å‰ã«ã‚¨ãƒ‡ãƒ¥ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ•™è‚²ï¼‰ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿæ–½ã—ã€ã‚¬ã‚¤ãƒ‰ã‚’ä»˜ã‘ã¦é‘‘è³ã™ã‚‹ã€‚ã¾ãŸã€å®‰å…¨ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚é‡‘å±æ¢çŸ¥æ©Ÿã«ã‚ˆã‚‹ãƒã‚§ãƒƒã‚¯ã‚‚è¡Œã†ã€‚ä¸è‡ªç”±å±•ã®ä¸­æ­¢å¾Œã€æŠ—è­°ã®æ„æ€ã‚’ç¤ºã™ãŸã‚ã€èŠ¸è¡“ç¥­ã«å‚åŠ ã—ã¦ã„ãŸä»–ã®å›½å†…å¤–10çµ„ä»¥ä¸Šã®ä½œå®¶ãŒå‡ºå±•ã‚’ä¸­æ­¢ãƒ»å¤‰æ›´ã—ã¦ã„ãŸãŒã€åŒå±•å†é–‹ã«ä¼´ã„ã€å…¨ä½œå®¶ãŒå¾©å¸°ã™ã‚‹ã€‚ä¸è‡ªç”±å±•ãŒé–‹å¹•3æ—¥ã§ä¸­æ­¢ã«ãªã£ãŸã“ã¨ã‚’å·¡ã£ã¦ã¯ã€æ–‡åŒ–åºãŒã€Œï¼ˆé–‹å‚¬ã«ã‚ˆã‚Šï¼‰å††æ»‘ãªé‹å–¶ãŒè„…ã‹ã•ã‚Œã‚‹ã“ã¨ã‚’èªè­˜ã—ã¦ã„ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ç”³å‘Šã—ãªã‹ã£ãŸã€ãªã©ã¨ã—ã¦ã€æ—¢ã«æ¡æŠã—ã¦ã„ãŸçœŒã¸ã®è£œåŠ©é‡‘ã®å…¨é¡ä¸äº¤ä»˜ã‚’æ±ºã‚ã¦ã„ã‚‹ã€‚èŠ¸è¡“ç¥­ã¯14æ—¥ã¾ã§ã€‚ã€ç«¹ç”°ç›´äººã€‘
     """
                
                let rubyString=tokenizer.addRubyTags(to: string, transliteration: .hiragana, options: [.kanjiOnly])
                XCTAssertFalse(rubyString.isEmpty)
                XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
            }
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
        
    }
    
    func testLong2(){
        
        /* this string has some of the hiragana composed of two characters instead of one, which throws off th erange finding algorithm. the work aroubnd is to restrict searching to tokens with kanji, which is what we care about anyway. If we want to get tokens from katakana words or hiragana expressions, the tokenizer will not produce these.
         The Solution is to use the precomposed string.
         */
        
        let string = """
"From: ç ”ç©¶ã‚µãƒ¼ãƒ’ã‚™ã‚¹ noreply@qemailserver.com Subject: ãƒ†ã‚¯ãƒãƒ­ã‚·ã‚™ãƒ¼ã«é–¢ã™ã‚‹ã“ã‚™æ„è¦‹ã‚’ãŠèã‹ã›ããŸã‚™ã•ã„\nDate: May 28, 2020 8:38\nTo:test\n   ç°¡å˜ãªã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã„ãŸãŸã‚™ã„ãŸæ–¹ã«5ãƒˆã‚™ãƒ«é€² å‘ˆã„ãŸã—ã¾ã™ã€‚\nç§ãŸã¡ã¯ã“ã®ç•°ä¾‹ã®å›°é›£ãªçŠ¶æ³ã«ã‚ã‚Šã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«å›ç­”ã—ã¦ã„ãŸãŸã‚™ãã“ã¨ ã¯ã€ã‚ãªãŸã«ã¨ã£ã¦å„ªå…ˆäº‹é …ã¦ã‚™ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã“ã¨ã‚’ç†è§£ã—ã¦ãŠã‚Šã¾ã™ã€‚ã— ã‹ã—ãªã‹ã‚™ã‚‰ã€å¼Šç¤¾ã«ã¨ã£ã¦ãŠå®¢æ§˜ã®å£°ã¯å¤§å¤‰è²´é‡ã¦ã‚™ã‚ã‚Šã€ã‚ãªãŸã®ã“ã‚™æ„è¦‹ã‚’ã›ã‚™\nã²ãŠèã‹ã›ã„ãŸãŸã‚™ããŸã„ã¨è€ƒãˆã¦ãŠã‚Šã¾ã™ã€‚\nã“ã®ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«ãŠç­”ãˆã„ãŸãŸã‚™ãã¾ã™ã¨ã€ 5ãƒˆã‚™ãƒ«ã®Amazonã‚­ã‚™ãƒ•ãƒˆã‚«ãƒ¼ãƒˆã‚™ ã‚’é€²å‘ˆ ã„ãŸã—ã¾ã™ã€‚ã‚­ã‚™ãƒ•ãƒˆã‚«ãƒ¼ãƒˆã‚™ã¯å‚åŠ æ¡ä»¶ã‚’æº€ãŸã—ã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã™ã¸ã‚™ã¦ã‚’å®Œäº†ã—ã¦\nã„ãŸãŸã‚™ã„ãŸã“ã‚™å‚åŠ è€…æ§˜ã«ã®ã¿è´ˆã‚‰ã›ã¦ã„ãŸãŸã‚™ãã¾ã™ã®ã¦ã‚™ã“ã‚™äº†æ‰¿ããŸã‚™ã•ã„ã€‚\nã“ã®æ©Ÿå¯†æ‰±ã„ã®èª¿æŸ»ã¯ãƒ†ã‚¯ãƒãƒ­ã‚·ã‚™ãƒ¼æ¥­ç•Œã®å¤§æ‰‹ä¼æ¥­ã¦ã‚™ã‚ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ä»£ã‚ ã‚Šã€Qualtricsã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ•ã‚šãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ä¸Šã¦ã‚™å®Ÿæ–½ã•ã‚Œã¾ã™ã€‚ã“ã®ä¼æ¥­ã®ã‚µãƒ¼ ãƒ’ã‚™ã‚¹ã«ã¤ã„ã¦ãŠä¼ºã„ã™ã‚‹ã‚‚ã®ã¦ã‚™ã™ã€‚ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã®æ‰€è¦æ™‚é–“ã¯20~25åˆ†ã»ã¨ã‚™ã¦ã‚™\nã™ã€‚\nä¸‹è¨˜ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã‹ã€ã‚¦ã‚§ãƒ•ã‚™ãƒ•ã‚™ãƒ©ã‚¦ã‚µã‚™ã«ãƒªãƒ³ã‚¯ã‚’ã‚³ãƒ’ã‚šãƒ¼ãƒ»ã‚¢ãƒ³ãƒˆã‚™ãƒ»ãƒ˜ã‚šãƒ¼ã‚¹ãƒˆ ã—ã¦å›ç­”ã‚’å§‹ã‚ã¦ããŸã‚™ã•ã„ã€‚\nã‚¯ãƒªãƒƒã‚¯\nã‚ã‚Šã‹ã‚™ã¨ã†ã“ã‚™ã•ã‚™ã„ã¾ã™ã€‚ã“ã‚™å”åŠ›ã«æ„Ÿè¬ã„ãŸã—ã¾ã™ã€‚\næœ¬ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã¯ä»»æ„ã¦ã‚™ã“ã‚™å‚åŠ ã„ãŸãŸã‚™ãã‚‚ã®ã¦ã‚™ã™ã€‚æ¡ä»¶ã‚’æº€ãŸã—ã€æœ¬ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã‚’å®Œäº†ã•ã‚ŒãŸ ã“ã‚™å‚åŠ è€…æ§˜ã¸ã€ã“ã®æ‹›å¾…çŠ¶ã‹ã‚™é€ä¿¡ã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«ã‚¢ãƒˆã‚™ãƒ¬ã‚¹å®›ã«ã‚­ã‚™ãƒ•ãƒˆã‚«ãƒ¼ãƒˆã‚™ã‹ã‚™é€ä¿¡ã•ã‚Œã¾ã™ã€‚\nå‡¦ç†ã«ã¯2~3é€±é–“ã‚’è¦ã—ã¾ã™ã®ã¦ã‚™ã“ã‚™äº†æ‰¿ããŸã‚™ã•ã„ã€‚å½“ç¤¾ã¯èª¿æŸ»ã‚’å®Ÿæ–½ã™ã‚‹ã“ã¨ã®ã¿ã‚’ç›®çš„ ã¨ã—ã¦ã“ã‚™é€£çµ¡ã•ã›ã¦ã„ãŸãŸã‚™ãã¾ã—ãŸã€‚æ°‘æ—æ€§ãªã¨ã‚™ã€åŸºæœ¬çš„ãªäººå£çµ±è¨ˆæƒ…å ±ã‚’ãŠå°‹ã­ã™ã‚‹å ´åˆ ã‹ã‚™ã‚ã‚Šã¾ã™ã€‚ã“ã‚™å‚åŠ è€…æ§˜ã®ãƒ†ã‚™ãƒ¼ã‚¿ã¯æ©Ÿå¯†æƒ…å ±ã¨ã—ã¦ä¿æŒã•ã‚Œã€ãƒ†ã‚¯ãƒãƒ­ã‚·ã‚™ãƒ¼æ¥­ç•Œã®å¤§æ‰‹ä¼æ¥­ ã¦ã‚™ã‚ã‚‹å½“ç¤¾ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨é›†è¨ˆãƒ†ã‚™ãƒ¼ã‚¿ã¨ã—ã¦ã®ã¿å…±æœ‰ã•ã‚Œã¾ã™ã€‚ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ã“ã®æƒ…å ± ã‚’ã‚µãƒ¼ãƒ’ã‚™ã‚¹ã®èª¿æŸ»ã®å®Ÿæ–½ãŠã‚ˆã²ã‚™ã‚µãƒ¼ãƒ’ã‚™ã‚¹ã®å‘ä¸Šã‚’ç›®çš„ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚æœ¬ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã‚’å®Œ\n "
"""
        
        
        
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
                XCTAssertNotNil(tokenizer)
                let tokens =  tokenizer.furiganaAnnotations(for: string, options: [.kanjiOnly])
                XCTAssert(tokens.count > 40)
                XCTAssert((tokens.first?.reading ?? "") == "ã‘ã‚“ãã‚…ã†")
                XCTAssert((tokens.last?.reading ?? "") == "ã‹ã‚“")
                XCTAssert((tokens.suffix(2).first?.reading ?? "")  == "ã»ã‚“")
            }
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
        
    }
    
    
    func testEmoji(){
        let text1="ç ”ç©¶ã‚µãƒ¼ãƒ’ã‚™ã‚¹ğŸ‡¯ğŸ‡µæ—¥æœ¬" //has decomposedã€€ãƒ“
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
                XCTAssertNotNil(tokenizer)
                let tokens =  tokenizer.furiganaAnnotations(for: text1, options: [.kanjiOnly])
                XCTAssert(tokens.isEmpty == false)
                XCTAssert((tokens.first?.reading ?? "") == "ã‘ã‚“ãã‚…ã†")
                XCTAssert((tokens.last?.reading ?? "") == "ã«ã£ã½ã‚“")
            }
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    
    func testHTML(){
        
        do{
            let currentURL=URL(fileURLWithPath: #file).deletingLastPathComponent().deletingLastPathComponent()
            let htmlURL=currentURL.appendingPathComponent("Resources", isDirectory: true).appendingPathComponent("helicobacter").appendingPathExtension("html")
            let htmlText=try String(contentsOf: htmlURL)
            
            let tokenizer=try Tokenizer(dictionary: IPADic())
            
            
            let rubyString=tokenizer.addRubyTags(to: htmlText, transliteration: .hiragana, options: [.kanjiOnly])
            XCTAssertFalse(rubyString.isEmpty)
            XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    
    
    func testPerformanceClassic(){
        
        let currentURL=URL(fileURLWithPath: #file).deletingLastPathComponent().deletingLastPathComponent()
        let htmlURL=currentURL.appendingPathComponent("Resources", isDirectory: true).appendingPathComponent("helicobacter").appendingPathExtension("html")
        
        measure {
            do{
                let htmlText=try String(contentsOf: htmlURL)
                let tokenizer=Tokenizer.systemTokenizer
                
                
                let rubyString=tokenizer.addRubyTags(to: htmlText, transliteration: .hiragana, options: [.kanjiOnly])
                XCTAssertFalse(rubyString.isEmpty)
                XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
            }
            catch let error{
                XCTFail(error.localizedDescription)
            }
        }
        
        
    }
    
    func testPerformanceStreamingMecab(){
        let currentURL=URL(fileURLWithPath: #file).deletingLastPathComponent().deletingLastPathComponent()
        let htmlURL=currentURL.appendingPathComponent("Resources", isDirectory: true).appendingPathComponent("ã‚¤ã‚­ã‚™ãƒªã‚¹").appendingPathExtension("html")
        measure {
            do{
                let htmlText=try String(contentsOf: htmlURL)
                let tokenizer=try Tokenizer(dictionary: IPADic())
                let rubyString=tokenizer.rubyTaggedString(source: htmlText, transliteration: .hiragana, options: [.kanjiOnly])
                XCTAssertFalse(rubyString.isEmpty)
                XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
            }
            catch let error{
                XCTFail(error.localizedDescription)
            }
        }
    }
    
    func testPerformanceStreamingMecab_transliterateAll(){
        let currentURL=URL(fileURLWithPath: #file).deletingLastPathComponent().deletingLastPathComponent()
        let htmlURL=currentURL.appendingPathComponent("Resources", isDirectory: true).appendingPathComponent("ã‚¤ã‚­ã‚™ãƒªã‚¹").appendingPathExtension("html")
        measure {
            do{
                let htmlText=try String(contentsOf: htmlURL)
                let tokenizer=try Tokenizer(dictionary: IPADic())
                let rubyString=tokenizer.rubyTaggedString(source: htmlText, transliteration: .romaji, options: [.kanjiOnly], transliterateAll: true)
                XCTAssertFalse(rubyString.isEmpty)
                XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
            }
            catch let error{
                XCTFail(error.localizedDescription)
            }
        }
    }
    
    
    func testPerformanceStreamingSystem(){
        let currentURL=URL(fileURLWithPath: #file).deletingLastPathComponent().deletingLastPathComponent()
        let htmlURL=currentURL.appendingPathComponent("Resources", isDirectory: true).appendingPathComponent("ã‚¤ã‚­ã‚™ãƒªã‚¹").appendingPathExtension("html")
        measure {
            do{
                let htmlText=try String(contentsOf: htmlURL)
                let tokenizer=Tokenizer.systemTokenizer
                let rubyString=tokenizer.rubyTaggedString(source: htmlText, transliteration: .romaji, options: [.kanjiOnly])
                XCTAssertFalse(rubyString.isEmpty)
                XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
            }
            catch let error{
                XCTFail(error.localizedDescription)
            }
        }
    }
    
    
    func testPerformanceStreamingSystem_transliterateAll(){
        let currentURL=URL(fileURLWithPath: #file).deletingLastPathComponent().deletingLastPathComponent()
        let htmlURL=currentURL.appendingPathComponent("Resources", isDirectory: true).appendingPathComponent("ã‚¤ã‚­ã‚™ãƒªã‚¹").appendingPathExtension("html")
        measure {
            do{
                let htmlText=try String(contentsOf: htmlURL)
                let tokenizer=Tokenizer.systemTokenizer
                let rubyString=tokenizer.rubyTaggedString(source: htmlText, transliteration: .romaji, options: [.kanjiOnly], transliterateAll: true)
                XCTAssertFalse(rubyString.isEmpty)
                XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
            }
            catch let error{
                XCTFail(error.localizedDescription)
            }
        }
        
    }
    
    
    func testTransliterateAll(){
        let text="ç†Šã®ãƒ—ãƒ¼ã•ã‚“ã®å¤§å¥½ç‰©ã¯ãƒãƒãƒŸãƒ„ã§ã™ã€‚ç†Šã®ãƒ—ãƒ¼ã•ã‚“ã¯è‹±èªã§Winni-The-Poohã¨å‘¼ã¶ã‚“ã§ã™ã€‚"
        let tokenizer=Tokenizer.systemTokenizer
        let rubyString=tokenizer.rubyTaggedString(source: text, transliteration: .romaji, options: [], transliterateAll: true)
        XCTAssertFalse(rubyString.range(of: "<ruby>")?.isEmpty ?? true)
        let mecab=try! Tokenizer(dictionary: IPADic())
        let rubyString_mecab=mecab.rubyTaggedString(source: text, transliteration: .romaji, options: [], transliterateAll: true)
        XCTAssertFalse(rubyString_mecab.range(of: "<ruby>")?.isEmpty ?? true)
        
        let text2="ã¼ããŸã¡ãŒã¨ã¦ã‚‚ã¡ã„ã•ã‹ã£ãŸã“ã‚"
        let ruby2=tokenizer.rubyTaggedString(source: text2, transliteration: .romaji, options: [], transliterateAll: true)
        XCTAssertFalse(ruby2.range(of: "<ruby>")?.isEmpty ?? true)
        let rubyString_mecab2=mecab.rubyTaggedString(source: text2, transliteration: .romaji, options: [], transliterateAll: true)
        XCTAssertFalse(rubyString_mecab2.range(of: "<ruby>")?.isEmpty ?? true)
    }
    
    func testTransliteration(){
        do{
            let tokenizers=try self.dictionaries.map({
                try Tokenizer(dictionary: $0)
            }) + [Tokenizer.systemTokenizer]
            
            for tokenizer in tokenizers{
                let text="ä¸–ç•Œäººå£"
                let furigana=tokenizer.furiganaAnnotations(for: text, transliteration: .hiragana, options: [.kanjiOnly])
                XCTAssertNotNil(furigana.first(where: {$0.reading == "ã˜ã‚“ã“ã†"}))
                
                let romaji=tokenizer.furiganaAnnotations(for: text, transliteration: .romaji, options: [.kanjiOnly])
                
                XCTAssertNotNil(romaji.first(where: {$0.reading == "jinkÅ"}))
            }
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    func testFilter1(){
        do{
            let text="ä¸–ç•Œäººå£"
            let tokenizer=try Tokenizer(dictionary: IPADic())
            let disallowed=["å£","äºº"]
            let furigana=tokenizer.furiganaAnnotations(for: text, transliteration: .hiragana, options: [.kanjiOnly, .filter(disallowedCharacters: Set(disallowed), strict: true)])
            XCTAssertNil(furigana.first(where: {$0.reading == "ã˜ã‚“ã“ã†"}))
            XCTAssertNotNil(furigana.first(where: {$0.reading == "ã›ã‹ã„"}))
            XCTAssert(furigana.count == 1)
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    func testFilter2(){
        do{
            let text="ä¸–ç•Œäººå£"
            let tokenizer=try Tokenizer(dictionary: IPADic())
            let disallowed=["å£"]
            let furigana=tokenizer.furiganaAnnotations(for: text, transliteration: .hiragana, options: [.kanjiOnly, .filter(disallowedCharacters: Set(disallowed), strict: false)])
            XCTAssertNotNil(furigana.first(where: {$0.reading == "ã˜ã‚“ã“ã†"}))
            XCTAssertNotNil(furigana.first(where: {$0.reading == "ã›ã‹ã„"}))
            XCTAssert(furigana.count == 2)
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
        
        
    }
    
    
    func testFilter3(){
        do{
            let text="ä¸–ç•Œäººå£"
            let tokenizer=try Tokenizer(dictionary: IPADic())
            let disallowed=SchoolYearFilter.elementary2.disallowedCharacters
            let furigana=tokenizer.furiganaAnnotations(for: text, transliteration: .hiragana, options: [.kanjiOnly, .filter(disallowedCharacters: disallowed, strict: false)])
            XCTAssertNil(furigana.first(where: {$0.reading == "ã˜ã‚“ã“ã†"}))
            XCTAssertNotNil(furigana.first(where: {$0.reading == "ã›ã‹ã„"}))
            XCTAssert(furigana.count == 1)
            
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
    }
    
    
    func testSystemTokenizer1(){
        
        let text="ä¸–ç•Œäººå£"
        let tokenizer=Tokenizer.systemTokenizer
        let disallowed=SchoolYearFilter.elementary2.disallowedCharacters
        let furigana=tokenizer.furiganaAnnotations(for: text, transliteration: .hiragana, options: [.kanjiOnly, .filter(disallowedCharacters: disallowed, strict: false)])
        XCTAssertNil(furigana.first(where: {$0.reading == "ã˜ã‚“ã“ã†"}))
        XCTAssertNotNil(furigana.first(where: {$0.reading == "ã›ã‹ã„"}))
        XCTAssert(furigana.count == 1)
    }
    
    func testShort_ruby(){
        do{
            let ipadicTokenizer=try Tokenizer(dictionary: IPADic())
            let system=Tokenizer.systemTokenizer
            let text="ã§ã‚‚é®­ã‚‚ã‚ˆãé£Ÿã¹ã¾ã™ã€‚"
            let rubyAnnotated=ipadicTokenizer.rubyTaggedString(source: text, transliteration: .hiragana, options: [.kanjiOnly, .filter(disallowedCharacters: Set(["é£Ÿ"]), strict: true)])
            XCTAssert(rubyAnnotated == "ã§ã‚‚<ruby>é®­<rt>ã•ã‘</rt></ruby>ã‚‚ã‚ˆãé£Ÿã¹ã¾ã™ã€‚")
            
            let systemAnnotated=system.rubyTaggedString(source: text, transliteration: .hiragana, options: [.kanjiOnly, .filter(disallowedCharacters: Set(["é£Ÿ"]), strict: true)])
            XCTAssert(systemAnnotated == "ã§ã‚‚<ruby>é®­<rt>ã•ã‘</rt></ruby>ã‚‚ã‚ˆãé£Ÿã¹ã¾ã™ã€‚")
                                                                                                  
            let long="ç†Šã®ãƒ—ãƒ¼ã•ã‚“ã®å¤§å¥½ç‰©ã¯ãƒãƒãƒŸãƒ„ã§ã™ã€‚ç†Šã®ãƒ—ãƒ¼ã•ã‚“ã¯è‹±èªã§Winni-The-Poohã¨å‘¼ã¶ã‚“ã§ã™ã€‚"
            
            let ruby=ipadicTokenizer.rubyTaggedString(source: long,
                                                      transliteration: .hiragana,
                                                      options: [
                                                        .kanjiOnly,
                                                        .filter(disallowedCharacters: CharacterFilter.schoolYear(year: .elementary3).disallowedCharacters, strict: true)
                                                      ])
            
            XCTAssert(ruby.isEmpty==false)
            
        }
        catch let error{
            XCTFail(error.localizedDescription)
        }
       
    }
    
    
    
    

    static var allTests = [
        ("Test Loading", testLoading),
    ]
}
